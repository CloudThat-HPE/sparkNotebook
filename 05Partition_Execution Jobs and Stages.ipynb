{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfb8731",
   "metadata": {},
   "source": [
    "###### Spark Transformations and actions results in execution plan. \n",
    "###### This execution plan finally execued by executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f38157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from lib.logger import Log4j\n",
    "from lib.utils import get_spark_app_config\n",
    "from lib.utils import load_survey_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc955ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = get_spark_app_config()\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0cc94e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://CT-LP-168:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[3]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HelloSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x200900f5940>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f599629",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Log4j(spark)\n",
    "logger.info(\"Starting HelloSpark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc33b2f",
   "metadata": {},
   "source": [
    "We are doing three thing here\n",
    "1. Reading csv and infereing schema - result is dataframe\n",
    "2. Applying Chain of Transformations - result is another dataframe\n",
    "3. we are using action show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b613fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = load_survey_df(spark,\"data/sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4b09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = survey_df.where(\"Age>40\").select(\"Age\",\"Gender\",\"Country\",\"state\").groupBy(\"Country\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3291a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Country|count|\n",
      "+-------------+-----+\n",
      "|United States|    2|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2cb8c",
   "metadata": {},
   "source": [
    "# Modular Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3320ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from lib.logger import Log4j\n",
    "from lib.utils import get_spark_app_config,load_survey_df,count_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e204b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = get_spark_app_config()\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "logger = Log4j(spark)\n",
    "logger.info(\"Starting HelloSpark\")\n",
    "survey_df = load_survey_df(spark,\"data/sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2932c",
   "metadata": {},
   "source": [
    "Based on this example we need to understand internal execution of plan.\n",
    "now csv file that we are reading and when it loaded as dataframe it have just one partition.\n",
    "Not good for learning real internal behaviour.\n",
    "###### Lets Simulate Multiple partions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23ec45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_df = survey_df.repartition(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fd09c",
   "metadata": {},
   "source": [
    "we partitioned survey_df into two partitions. will use for further transformations.\n",
    "\n",
    "now we are appluying groupBy as wide transformation so it cause internal partitions.\n",
    "\n",
    "we created two forced partitions on dataframe by applying reparttion transformation but we cannot have control over parttion caused internally by suffle sort to handle wide transformations.\n",
    "\n",
    "to control this behaviour will use configuration. spark.sql.shuffle.partition property will set number of partition. add this property value in spark.conf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aead75bc",
   "metadata": {},
   "source": [
    "Replace show() with collect() action and write info to logger\n",
    "collect action returns the dataframe as python list. show method is utilty function to print the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0761ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = count_by_country(partitioned_df)\n",
    "logger.info(count_df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164c459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press Enter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input(\"Press Enter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71aea3c",
   "metadata": {},
   "source": [
    "We are executing this execution plan locally and look at spark UI to get details. Spark UI only available during life of an application. so i am going to ask user input here in above code cell. so till we eneter somethinf application is live and will visit SPARK UI to get execution plan.\n",
    "But remember to remove this line later. because this line is forlocal debugging not for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cfa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eff944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8e2474611b83d9571a280a7ba862d36318511be3482fc96112f8467724a795a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
