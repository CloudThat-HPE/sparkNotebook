{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfcfd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from lib.logger import Log4j\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[3]\").appName(\"Read Formats using API\").getOrCreate()\n",
    "logger = Log4j(spark)\n",
    "logger.info(\"Starting HelloSparkSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c1c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeCsvDf = spark.read \\\n",
    "                    .format(\"csv\") \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .load(\"data/flight*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d72ef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "| FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
      "+--------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|1/1/2000|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
      "|1/1/2000|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
      "|1/1/2000|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
      "|1/1/2000|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
      "|1/1/2000|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
      "+--------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightTimeCsvDf.show(5)\n",
    "logger.info(\"CSV Schema:\" + flightTimeCsvDf.schema.simpleString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3b6d62",
   "metadata": {},
   "source": [
    "reader API correctly reading column from header correctly, hwever datatype for each column is string.\n",
    "What if we infered the schema. lets try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca06671",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeCsvDf = spark.read \\\n",
    "                    .format(\"csv\") \\\n",
    "                    .option(\"header\", \"true\") \\\n",
    "                    .option(\"inferSchema\",\"true\")\\\n",
    "                    .load(\"data/flight*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e9d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "| FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
      "+--------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|1/1/2000|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
      "|1/1/2000|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
      "|1/1/2000|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
      "|1/1/2000|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
      "|1/1/2000|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
      "+--------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightTimeCsvDf.show(5)\n",
    "logger.info(\"CSV Schema:\" + flightTimeCsvDf.schema.simpleString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917230a7",
   "metadata": {},
   "source": [
    "Now it is little better, numeric field infered to be an integer, However date field is still a string.\n",
    "point is stregth- you cannot rely on schema infered option.\n",
    "so you have only two options here\n",
    "1. explicit - explicitly set schema for dataframe or\n",
    "2. use dataframe that comes with schema implicit schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc15c1",
   "metadata": {},
   "source": [
    "###### Lets Read json file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0398ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "|ARR_TIME|CANCELLED|CRS_ARR_TIME|CRS_DEP_TIME|DEP_TIME|DEST|DEST_CITY_NAME|DISTANCE| FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|TAXI_IN|WHEELS_ON|\n",
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "|    1348|        0|        1400|        1115|    1113| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1451|   BOS|      Boston, MA|      5|     1343|\n",
      "|    1543|        0|        1559|        1315|    1311| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1479|   BOS|      Boston, MA|      7|     1536|\n",
      "|    1651|        0|        1721|        1415|    1414| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1857|   BOS|      Boston, MA|      9|     1642|\n",
      "|    2005|        0|        2013|        1715|    1720| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1997|   BOS|      Boston, MA|     10|     1955|\n",
      "|    2240|        0|        2300|        2015|    2010| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             2065|   BOS|      Boston, MA|     10|     2230|\n",
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightTimeJsonDf = spark.read \\\n",
    "                    .format(\"json\") \\\n",
    "                    .load(\"data/flight*.json\")\n",
    "flightTimeJsonDf.show(5)\n",
    "logger.info(\"Json Schema:\" + flightTimeJsonDf.schema.simpleString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a296adc6",
   "metadata": {},
   "source": [
    "problem still remain as it is. as jason do not come with header and for json bydefault schema is infered by reader API so we removed option method for header and infer schema. but still date type is string.\n",
    "\n",
    "So we have only otion to set expplicit schema.\n",
    "\n",
    "Before that read file which come with schema viz. parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b9ef2",
   "metadata": {},
   "source": [
    "###### Lets Read parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eb94e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "|ARR_TIME|CANCELLED|CRS_ARR_TIME|CRS_DEP_TIME|DEP_TIME|DEST|DEST_CITY_NAME|DISTANCE| FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|TAXI_IN|WHEELS_ON|\n",
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "|    1348|        0|        1400|        1115|    1113| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1451|   BOS|      Boston, MA|      5|     1343|\n",
      "|    1543|        0|        1559|        1315|    1311| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1479|   BOS|      Boston, MA|      7|     1536|\n",
      "|    1651|        0|        1721|        1415|    1414| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1857|   BOS|      Boston, MA|      9|     1642|\n",
      "|    2005|        0|        2013|        1715|    1720| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1997|   BOS|      Boston, MA|     10|     1955|\n",
      "|    2240|        0|        2300|        2015|    2010| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             2065|   BOS|      Boston, MA|     10|     2230|\n",
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightTimeParquetDf = spark.read \\\n",
    "                    .format(\"parquet\") \\\n",
    "                    .load(\"data/flight*.parquet\")\n",
    "flightTimeParquetDf.show(5)\n",
    "logger.info(\"Parquet Schema:\" + flightTimeParquetDf.schema.simpleString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47aacd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2122277",
   "metadata": {},
   "source": [
    "Well Done. Everything is perfect.\n",
    "\n",
    "Point is stregth - use parquet format as long as it is possible. It is recommended and default file format for apache spark. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
