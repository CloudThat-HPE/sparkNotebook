{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905ec6e3",
   "metadata": {},
   "source": [
    "We are creating avro output in this example.\n",
    "avro doesnt bundled with apache spark.\n",
    "if you want to work with it then you must include scala avro package.\n",
    "use spark-default.conf file to add pckages.\n",
    "spark.jars.packages\t\t\t       org.apache.spark:spark-avro_2.11:2.4.5\t\n",
    "\n",
    "###### We will be using paruet file as source and creating avro file as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e182292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from lib.logger import Log4j\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[3]\").appName(\"Read Formats using API\").getOrCreate()\n",
    "logger = Log4j(spark)\n",
    "logger.info(\"Starting HelloSparkSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7ae882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
      "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
      "|2000-01-01|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
      "|2000-01-01|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
      "|2000-01-01|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightTimeParquetDf = spark.read \\\n",
    "                    .format(\"parquet\") \\\n",
    "                    .load(\"data/flight*.parquet\")\n",
    "flightTimeParquetDf.show(5)\n",
    "logger.info(\"Parquet Schema:\" + flightTimeParquetDf.schema.simpleString())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851222c",
   "metadata": {},
   "source": [
    "lets not do any data processing write the file in avro format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721bc013",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeParquetDf.write \\\n",
    "        .format(\"avro\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"path\", \"dataSink/avro/\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e95a2",
   "metadata": {},
   "source": [
    "How many output file are you getting? 1 2 3 4..?\n",
    "well, those many partitions does flightTimeParquetDF have those many file we are getting in output folder avro.\n",
    "if you have 5 parttions and 5 executors then each parttion will be written as output file by each executor parellely.\n",
    "if you have 5 parttions and 3 executors the 2+2+1 parttions will be written by 3 executors parellely.\n",
    "\n",
    "But how many parttions do i have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d0b657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Num Partitions before: \" + str(flightTimeParquetDf.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735291bd",
   "metadata": {},
   "source": [
    "2 partition will be getting 1 avro file only.? and also there are some other file as well? before answer this let explain each output file we are getting \n",
    "avro directory containing three files\n",
    "1. avro - Datafile\n",
    "2. crc file which hold datafile checksum\n",
    "3. success file indicates write operation successfull\n",
    "\n",
    "we have 2 parttions but we are getting only 1 file.\n",
    "\n",
    "we know parttions but we do not know how many record does we have in each partition?\n",
    "count no of roes in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "227f742e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|SPARK_PARTITION_ID()| count|\n",
      "+--------------------+------+\n",
      "|                   0|470477|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "flightTimeParquetDf.groupBy(spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023d0c8d",
   "metadata": {},
   "source": [
    "Parttion id 0 get all record and other parttion do not have any record. that why we get only one file as an output though we have 2 parttion.\n",
    "\n",
    "##### Lets reparttion it and lets see will get multiple file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c3e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitionedDF = flightTimeParquetDf.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54a01b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|SPARK_PARTITION_ID()|count|\n",
      "+--------------------+-----+\n",
      "|                   1|94096|\n",
      "|                   3|94095|\n",
      "|                   4|94095|\n",
      "|                   2|94096|\n",
      "|                   0|94095|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Num Partitions after: \" + str(partitionedDF.rdd.getNumPartitions()))\n",
    "partitionedDF.groupBy(spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9babca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitionedDF.write \\\n",
    "        .format(\"avro\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"path\", \"dataSink/avro/\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a6ae2",
   "metadata": {},
   "source": [
    "We evenely distributed record among five partition and then we get five avro files as well. But even distribution is not so good because it provides parellel processing but hampers parttion elimination.\n",
    "to avoid this you may partion data basis on some column or set of column would work.\n",
    "use partitionBy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4000c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeParquetDf.write \\\n",
    "        .format(\"avro\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"path\", \"dataSink/avro/\") \\\n",
    "        .partitionBy(\"OP_CARRIER\", \"ORIGIN\") \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5135e3",
   "metadata": {},
   "source": [
    "largest file we are getting with maximum no of record. how are we controlling file size? by controlling no of records in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9abee304",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeParquetDf.write \\\n",
    "        .format(\"avro\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"path\", \"dataSink/avro/\") \\\n",
    "        .partitionBy(\"OP_CARRIER\", \"ORIGIN\") \\\n",
    "        .option(\"maxRecordsPerFile\", 10000) \\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "758680ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e148a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
