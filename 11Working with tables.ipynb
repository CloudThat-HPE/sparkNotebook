{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc5811c",
   "metadata": {},
   "source": [
    "##### Create Manage Table and access the catlog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552792b2",
   "metadata": {},
   "source": [
    "creating managed table need persistent metastore. apache spark depends on hive metastore. so we need hive for this example.  \n",
    "\n",
    "lets load sparksession and read parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9e193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from lib.logger import Log4j\n",
    "\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .master(\"local[3]\") \\\n",
    "            .appName(\"Read Formats using API\") \\\n",
    "            .enableHiveSupport() \\\n",
    "            .getOrCreate()\n",
    "\n",
    "logger = Log4j(spark)\n",
    "logger.info(\"Starting HelloSparkSQL\")\n",
    "\n",
    "flightTimeParquetDF = spark.read \\\n",
    "        .format(\"parquet\") \\\n",
    "        .load(\"data/flight*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8e41f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
      "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
      "|2000-01-01|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
      "|2000-01-01|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
      "|2000-01-01|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightTimeParquetDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea86b7",
   "metadata": {},
   "source": [
    "Lets not do any data processing. as we need to learn mechanics of saving dataframe into managed tables.\n",
    "\n",
    "why i do not save it as paruqet file? why we need to save it as managed table?\n",
    "when you want to access datafile from source, You may read datafile using reader api. after processing yu may then save it as from dataframe as datafile in parquet format using writer api. what if you want to make this data available to sql compint tool? you require it to store as tables. so tools can access it over jdbc/odbc connectors.\n",
    "table you can access through JDBC/ODBC connectors by various tools like tablue, talend, powerbi etc.\n",
    "parquet, json, avro not accesible through JDBC/ODBC connectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d48e20",
   "metadata": {},
   "source": [
    "use save AsTable method, it takes table name and creates managed table and store table to current database. but what is current database?\n",
    "Apache Spark come with one default database, and database name itself is default. if you want to store it in different database then you have two options\n",
    "1. prefix table name with database name\n",
    "2. access catlog and set current database for this session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4cff04",
   "metadata": {},
   "source": [
    "lets create database AIRLINE_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW DATABASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630828a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS AIRLINE_DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984df71",
   "metadata": {},
   "source": [
    "method 1 prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f31b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeParquetDF.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"AIRLINE_DB.flight_data_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac11c486",
   "metadata": {},
   "source": [
    "method 2 set current database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8dfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase(\"AIRLINE_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5627b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeParquetDF.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"flight_data_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10eaabf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='flight_data_tbl', database='airline_db', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info(spark.catalog.listTables(\"AIRLINE_DB\"))\n",
    "spark.catalog.listTables(\"AIRLINE_DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0fa94ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
      "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
      "|2000-01-01|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
      "|2000-01-01|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
      "|2000-01-01|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
      "|2000-01-01|        US|             2619|   BOS|      Boston, MA| ATL|   Atlanta, GA|         650|     649|      956|      7|         955|    1003|        0|     946|\n",
      "|2000-01-01|        US|             2621|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1440|    1446|     1713|      4|        1738|    1717|        0|     946|\n",
      "|2000-01-01|        DL|              346|   BTR| Baton Rouge, LA| ATL|   Atlanta, GA|        1740|    1744|     1957|      9|        2008|    2006|        0|     449|\n",
      "|2000-01-01|        DL|              412|   BTR| Baton Rouge, LA| ATL|   Atlanta, GA|        1345|    1345|     1552|      9|        1622|    1601|        0|     449|\n",
      "|2000-01-01|        DL|              299|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|        1245|    1245|     1443|      5|        1455|    1448|        0|     712|\n",
      "|2000-01-01|        DL|              495|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|        2035|    2035|     2226|      9|        2241|    2235|        0|     712|\n",
      "|2000-01-01|        DL|              677|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|         710|     710|      940|      7|         925|     947|        0|     712|\n",
      "|2000-01-01|        DL|              251|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        2040|    2100|     2235|      7|        2233|    2242|        0|     576|\n",
      "|2000-01-01|        DL|             1003|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1635|    1838|     2020|     12|        1832|    2032|        0|     576|\n",
      "|2000-01-01|        DL|             1501|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1430|    1435|     1623|     12|        1634|    1635|        0|     576|\n",
      "|2000-01-01|        DL|             1907|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|         530|     530|      716|      4|         723|     720|        0|     576|\n",
      "|2000-01-01|        DL|             2063|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1250|    null|     null|   null|        1449|    null|        1|     576|\n",
      "|2000-01-01|        DL|             2111|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1845|    1855|     2041|      9|        2046|    2050|        0|     576|\n",
      "|2000-01-01|        US|             2632|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|         710|     710|     null|   null|         905|    null|        0|     576|\n",
      "|2000-01-01|        US|             2967|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1700|    1700|     1845|      6|        1853|    1851|        0|     576|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM flight_data_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d125c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  470477|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM flight_data_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dcdd13",
   "metadata": {},
   "source": [
    "When you are working in local mode: these directories created in your local directory\n",
    "When you working in cluster mode: it would configure by cluster admin and it would be in common location across your all spark applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b14cfa",
   "metadata": {},
   "source": [
    "However we created table. lets create partitioned table. add partitionedBy method. i am using two columns to partition my table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d72cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeParquetDF.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"OP_CARRIER\", \"ORIGIN\") \\\n",
    "        .option(\"path\", \"dataSink/parquet/\") \\\n",
    "        .saveAsTable(\"flight_data_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cebf0ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  470477|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM flight_data_tbl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ac5e7",
   "metadata": {},
   "source": [
    "Great i get more parttions say more than 100 or 200. however for large dataset still these 200+ parttions are fine but what if i have 10000 unique values, do you want to create 10k partitions?\n",
    "absolutely not right?\n",
    "so do not parttion your table that has got to many unique value.\n",
    "Instead you can use bucketby.\n",
    "The bucketBy allows you to restrict no of parttions\n",
    "bucketBy(5, ) - limits to five partitions know as buckets. still i want to parttion on same two columns, add them\n",
    "bucketBy(5, \"OP_CARRIER\", \"ORIGIN\" )\n",
    "change to csv - as parquet is binary format, and to manually investigate data changing format purposefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0249f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeParquetDF.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .bucketBy(5, \"OP_CARRIER\", \"ORIGIN\") \\\n",
    "        .option(\"path\", \"dataSink/csv/\") \\\n",
    "        .saveAsTable(\"flight_data_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c306a1a",
   "metadata": {},
   "source": [
    "i got five parttions. backeting does not require lengthy directory structure. it is as simple as data files.\n",
    "but how it happened?\n",
    "I asked to create 5 buckets.\n",
    "Spark created five files, each file is one bucket.\n",
    "Now spark read one record, look at key column values, in our case it is op_carrier and origin, now spark will compute hash value, \n",
    "you got some hash number, divide it by 5 and take reminder, you are getting something 0-4, if number is 0 place record in first file.. and so on and soforth for each record.(there are various hashing method s are but basic principle is the same)\n",
    "Each unique key going to produce same hash value and they are in one file.\n",
    "these bucket improves join operation and if sorted then much more useful in certain operations. so bucketby also have sort by companion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebcfc6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightTimeParquetDF.write \\\n",
    "        .format(\"csv\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .bucketBy(5, \"OP_CARRIER\", \"ORIGIN\") \\\n",
    "        .sortBy(\"OP_CARRIER\", \"ORIGIN\") \\\n",
    "        .option(\"path\", \"dataSink/csv/\") \\\n",
    "        .saveAsTable(\"flight_data_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bdc0908",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
